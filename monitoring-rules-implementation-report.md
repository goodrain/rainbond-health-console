# Rainbond å¹³å°ç¨³å®šæ€§ç›‘æ§è§„åˆ™å®ç°æŠ¥å‘Š

**ç”Ÿæˆæ—¶é—´**: 2025-12-19
**é¡¹ç›®**: rainbond-health-console
**çŠ¶æ€**: âœ… æ‰€æœ‰è§„åˆ™å·²å®Œæ•´å®ç°

---

## ç›®å½•

- [ä¸€ã€æ¦‚è¿°](#ä¸€æ¦‚è¿°)
- [äºŒã€P0 çº§åˆ«ç›‘æ§ï¼ˆè‡´å‘½ï¼‰](#äºŒp0-çº§åˆ«ç›‘æ§è‡´å‘½)
  - [1. æ•°æ®åº“è¿æ¥ç›‘æ§](#1-æ•°æ®åº“è¿æ¥ç›‘æ§)
  - [2. Kubernetes é›†ç¾¤ç›‘æ§](#2-kubernetes-é›†ç¾¤ç›‘æ§)
  - [3. å®¹å™¨é•œåƒä»“åº“ç›‘æ§](#3-å®¹å™¨é•œåƒä»“åº“ç›‘æ§)
  - [4. å¯¹è±¡å­˜å‚¨ç›‘æ§](#4-å¯¹è±¡å­˜å‚¨ç›‘æ§)
- [ä¸‰ã€æŠ€æœ¯æ¶æ„æ€»ç»“](#ä¸‰æŠ€æœ¯æ¶æ„æ€»ç»“)
- [å››ã€é…ç½®æŒ‡å—](#å››é…ç½®æŒ‡å—)
- [äº”ã€å‘Šè­¦è§„åˆ™ç¤ºä¾‹](#äº”å‘Šè­¦è§„åˆ™ç¤ºä¾‹)

---

## ä¸€ã€æ¦‚è¿°

æœ¬é¡¹ç›®æ˜¯ä¸€ä¸ªä¸“ä¸º Rainbond å¹³å°è®¾è®¡çš„ Kubernetes åŸç”Ÿç›‘æ§æœåŠ¡ï¼Œèšç„¦äºæ ¸å¿ƒåŸºç¡€è®¾æ–½çš„å¯ç”¨æ€§ç›‘æ§ã€‚

### 1.1 å®ç°çŠ¶æ€æ€»è§ˆ

| ç›‘æ§çº§åˆ« | è§„åˆ™æ•°é‡ | å®ç°çŠ¶æ€ | å®ç°ç‡ |
|---------|---------|---------|--------|
| P0 çº§åˆ«ï¼ˆè‡´å‘½ï¼‰ | 7 é¡¹ | 7 é¡¹å®Œæˆ | 100% |
| **æ€»è®¡** | **7 é¡¹** | **7 é¡¹å®Œæˆ** | **100%** |

**è¯´æ˜**: èµ„æºçº§åˆ«ç›‘æ§ï¼ˆç£ç›˜ç©ºé—´ã€è®¡ç®—èµ„æºã€èŠ‚ç‚¹çŠ¶æ€ï¼‰å·²è¿ç§»è‡³ node-exporter ç»Ÿä¸€æ”¶é›†ã€‚

### 1.2 æ¶æ„è®¾è®¡

**æ ¸å¿ƒç‰¹ç‚¹**ï¼š
- **é‡‡é›†å™¨æ¨¡å¼**: æ¯ä¸ªç›‘æ§åŠŸèƒ½ç‹¬ç«‹æˆé‡‡é›†å™¨ï¼Œè¿è¡Œåœ¨ç‹¬ç«‹ goroutine
- **Prometheus é›†æˆ**: ä½¿ç”¨æ ‡å‡† Prometheus metrics æ ¼å¼æš´éœ²æŒ‡æ ‡
- **äº‘åŸç”Ÿ**: å®Œå…¨é€šè¿‡ç¯å¢ƒå˜é‡é…ç½®ï¼Œé€‚é… Kubernetes éƒ¨ç½²
- **é«˜å¯ç”¨**: é‡‡é›†å™¨é—´äº’ä¸å½±å“ï¼Œå•ä¸ªæ•…éšœä¸å½±å“å…¶ä»–ç›‘æ§

**æŠ€æœ¯æ ˆ**ï¼š
- è¯­è¨€: Go 1.21+
- K8s å®¢æˆ·ç«¯: client-go v0.28.4
- Metrics å®¢æˆ·ç«¯: k8s.io/metrics v0.28.4
- Prometheus å®¢æˆ·ç«¯: prometheus/client_golang v1.23.2
- æ•°æ®åº“é©±åŠ¨: go-sql-driver/mysql v1.9.3
- å¯¹è±¡å­˜å‚¨ SDK: minio-go/v7 v7.0.97

**é¡¹ç›®ç»“æ„**ï¼š
```
rainbond-health-console/
â”œâ”€â”€ main.go                    # ç¨‹åºå…¥å£ï¼Œç”Ÿå‘½å‘¨æœŸç®¡ç†
â”œâ”€â”€ config/config.go           # é…ç½®ç®¡ç†ï¼ˆç¯å¢ƒå˜é‡åŠ è½½ï¼‰
â”œâ”€â”€ metrics/metrics.go         # Prometheus æŒ‡æ ‡å®šä¹‰
â””â”€â”€ collectors/                # ç›‘æ§é‡‡é›†å™¨å®ç°
    â”œâ”€â”€ database.go            # æ•°æ®åº“ç›‘æ§
    â”œâ”€â”€ kubernetes.go          # K8s é›†ç¾¤ç›‘æ§
    â”œâ”€â”€ registry.go            # é•œåƒä»“åº“ç›‘æ§
    â””â”€â”€ storage.go             # å¯¹è±¡å­˜å‚¨ç›‘æ§
```

---

## äºŒã€P0 çº§åˆ«ç›‘æ§ï¼ˆè‡´å‘½ï¼‰

**å½±å“**: å¹³å°æ ¸å¿ƒåŠŸèƒ½ä¸å¯ç”¨

### 1. æ•°æ®åº“è¿æ¥ç›‘æ§

#### 1.1 è§„åˆ™å®šä¹‰

| ç›‘æ§æŒ‡æ ‡ | å‘Šè­¦æ¡ä»¶ | è¯´æ˜ |
|---------|---------|------|
| å†…éƒ¨æ•°æ®åº“è¿æ¥ | mysql_up == 0 | æ•°æ®åº“ä¸å¯è¾¾ |
| å¤–éƒ¨æ•°æ®åº“è¿æ¥ | mysql_up == 0 | æ•°æ®åº“ä¸å¯è¾¾ |

#### 1.2 å®ç°çŠ¶æ€

âœ… **å·²å®Œæ•´å®ç°**

**å®ç°æ–‡ä»¶**: `collectors/database.go:1`

#### 1.3 å®ç°åŸç†

```go
type DatabaseCollector struct {
    databases []config.DatabaseConfig  // æ”¯æŒå¤šä¸ª MySQL å®ä¾‹
    interval  time.Duration             // é‡‡é›†é—´éš”ï¼ˆé»˜è®¤ 30sï¼‰
    ctx       context.Context
    cancel    context.CancelFunc
}
```

**å·¥ä½œæµç¨‹**ï¼š

1. **é…ç½®åŠ è½½**: ä»ç¯å¢ƒå˜é‡åŠ è½½å¤šä¸ªæ•°æ®åº“é…ç½®
   ```bash
   DB_1_NAME=internal
   DB_1_HOST=mysql.database.svc.cluster.local
   DB_1_PORT=3306
   DB_1_USER=root
   DB_1_PASSWORD=password
   DB_1_DATABASE=mysql

   DB_2_NAME=external
   DB_2_HOST=external-mysql.example.com
   ...
   ```

2. **è¿æ¥æ£€æµ‹**: ä½¿ç”¨ `database/sql` æ ‡å‡†åº“
   ```go
   dsn := fmt.Sprintf("%s:%s@tcp(%s:%d)/%s?timeout=5s",
       dbConfig.Username, dbConfig.Password,
       dbConfig.Host, dbConfig.Port, dbConfig.Database)

   db, err := sql.Open("mysql", dsn)
   defer db.Close()

   // 5 ç§’è¶…æ—¶ï¼Œé¿å…é•¿æ—¶é—´é˜»å¡
   ctx, cancel := context.WithTimeout(context.Background(), 5*time.Second)
   defer cancel()

   if err := db.PingContext(ctx); err != nil {
       metrics.MySQLUp.WithLabelValues(dbConfig.Name).Set(0)  // ä¸å¯ç”¨
   } else {
       metrics.MySQLUp.WithLabelValues(dbConfig.Name).Set(1)  // æ­£å¸¸
   }
   ```

3. **å®šæœŸé‡‡é›†**: æ¯ 30 ç§’æ‰§è¡Œä¸€æ¬¡æ£€æµ‹

**æš´éœ²æŒ‡æ ‡**ï¼š
```prometheus
# HELP mysql_up MySQL database availability (1=up, 0=down)
# TYPE mysql_up gauge
mysql_up{instance="internal"} 1
mysql_up{instance="external"} 1
```

**å…³é”®è®¾è®¡**ï¼š
- â±ï¸ 5 ç§’è¶…æ—¶ï¼Œé¿å…ç½‘ç»œæ•…éšœæ—¶é•¿æ—¶é—´é˜»å¡
- ğŸ”„ æ”¯æŒå¤šæ•°æ®åº“å®ä¾‹ï¼Œé€šè¿‡ `instance` æ ‡ç­¾åŒºåˆ†
- ğŸ›¡ï¸ æ¯æ¬¡æ£€æµ‹åç«‹å³å…³é—­è¿æ¥ï¼Œä¸ç»´æŠ¤é•¿è¿æ¥

---

### 2. Kubernetes é›†ç¾¤ç›‘æ§

#### 2.1 è§„åˆ™å®šä¹‰

| ç›‘æ§æŒ‡æ ‡ | å‘Šè­¦æ¡ä»¶ | è¯´æ˜ |
|---------|---------|------|
| API Server | apiserver ä¸å¯è¾¾ | K8s API ä¸å¯ç”¨ |
| CoreDNS | DNS è§£æå¤±è´¥ | é›†ç¾¤å†…éƒ¨åŸŸåè§£æå¼‚å¸¸ |
| Etcd | etcd é›†ç¾¤ä¸å¯ç”¨ | K8s å­˜å‚¨åç«¯æ•…éšœ |
| é›†ç¾¤å­˜å‚¨ | å¤–éƒ¨å­˜å‚¨ç±»ä¸å¯ç”¨ | æ— æ³•åˆ›å»º PVC |

#### 2.2 å®ç°çŠ¶æ€

âœ… **å·²å®Œæ•´å®ç°**

**å®ç°æ–‡ä»¶**: `collectors/kubernetes.go:1`

#### 2.3 å®ç°åŸç†

##### 2.3.1 API Server ç›‘æ§

**å®ç°ä»£ç **ï¼š
```go
func (c *KubernetesCollector) checkAPIServer() {
    ctx, cancel := context.WithTimeout(context.Background(), 10*time.Second)
    defer cancel()

    // è°ƒç”¨ ServerVersion() æ£€æµ‹ API Server å¯è¾¾æ€§
    _, err := c.clientset.Discovery().ServerVersion()
    if err != nil {
        metrics.KubernetesAPIServerUp.Set(0)
        log.Printf("[ERROR] Kubernetes API Server check failed: %v", err)
    } else {
        metrics.KubernetesAPIServerUp.Set(1)
    }
}
```

**åŸç†**ï¼š
- ä½¿ç”¨ K8s client-go çš„ Discovery æ¥å£
- è°ƒç”¨ `ServerVersion()` æ–¹æ³•éªŒè¯ API Server å¯è¾¾æ€§
- 10 ç§’è¶…æ—¶ä¿æŠ¤

**æš´éœ²æŒ‡æ ‡**ï¼š
```prometheus
# HELP kubernetes_apiserver_up Kubernetes API Server availability (1=up, 0=down)
# TYPE kubernetes_apiserver_up gauge
kubernetes_apiserver_up 1
```

---

##### 2.3.2 CoreDNS ç›‘æ§

**å®ç°ä»£ç **ï¼š
```go
func (c *KubernetesCollector) checkCoreDNS() {
    ctx, cancel := context.WithTimeout(context.Background(), 10*time.Second)
    defer cancel()

    // æ­¥éª¤ 1: æ£€æŸ¥ CoreDNS Pod æ˜¯å¦å­˜åœ¨
    pods, err := c.clientset.CoreV1().Pods("kube-system").List(ctx, metav1.ListOptions{
        LabelSelector: "k8s-app=kube-dns",  // CoreDNS Pod æ ‡ç­¾
    })
    if err != nil || len(pods.Items) == 0 {
        metrics.CoreDNSUp.Set(0)
        return
    }

    // æ­¥éª¤ 2: æ£€æŸ¥æ˜¯å¦æœ‰ Running ä¸” Ready çš„ Pod
    hasReadyPod := false
    for _, pod := range pods.Items {
        if pod.Status.Phase == corev1.PodRunning {
            // æ£€æŸ¥ PodReady æ¡ä»¶
            for _, condition := range pod.Status.Conditions {
                if condition.Type == corev1.PodReady && condition.Status == corev1.ConditionTrue {
                    hasReadyPod = true
                    break
                }
            }
        }
    }

    if !hasReadyPod {
        metrics.CoreDNSUp.Set(0)
        return
    }

    // æ­¥éª¤ 3: å®é™…æ‰§è¡Œ DNS è§£ææµ‹è¯•
    _, err = net.LookupHost("kubernetes.default.svc.cluster.local")
    if err != nil {
        metrics.CoreDNSUp.Set(0)
        log.Printf("[ERROR] CoreDNS resolution test failed: %v", err)
    } else {
        metrics.CoreDNSUp.Set(1)
    }
}
```

**åŸç†**ï¼ˆä¸‰å±‚æ£€æµ‹ï¼‰ï¼š
1. **Pod å­˜åœ¨æ€§**: æ£€æŸ¥ `kube-system` å‘½åç©ºé—´ä¸­æ˜¯å¦æœ‰ CoreDNS Pod
2. **Pod å°±ç»ªæ€§**: éªŒè¯è‡³å°‘ä¸€ä¸ª Pod å¤„äº Running å’Œ Ready çŠ¶æ€
3. **åŠŸèƒ½éªŒè¯**: å®é™…è§£æ `kubernetes.default.svc.cluster.local` åŸŸå

**æš´éœ²æŒ‡æ ‡**ï¼š
```prometheus
# HELP coredns_up CoreDNS availability (1=up, 0=down)
# TYPE coredns_up gauge
coredns_up 1
```

---

##### 2.3.3 Etcd ç›‘æ§

**å®ç°ä»£ç **ï¼š
```go
func (c *KubernetesCollector) checkEtcd() {
    ctx, cancel := context.WithTimeout(context.Background(), 10*time.Second)
    defer cancel()

    // æ­¥éª¤ 1: æŸ¥æ‰¾ etcd Podï¼ˆé€‚ç”¨äº kubeadm éƒ¨ç½²ï¼‰
    pods, err := c.clientset.CoreV1().Pods("kube-system").List(ctx, metav1.ListOptions{
        LabelSelector: "component=etcd",
    })

    // æ­¥éª¤ 2: å¦‚æœæ²¡æœ‰ Podï¼ˆå¤–éƒ¨ etcdï¼‰ï¼Œé€šè¿‡ API Server å¥åº·æ£€æŸ¥é—´æ¥éªŒè¯
    if err != nil || len(pods.Items) == 0 {
        // Etcd å¯èƒ½è¿è¡Œåœ¨é›†ç¾¤å¤–ï¼Œé€šè¿‡ API Server çš„ livez ç«¯ç‚¹æ£€æµ‹
        req := c.clientset.Discovery().RESTClient().Get().AbsPath("/livez")
        result := req.Do(ctx)
        if err := result.Error(); err != nil {
            metrics.EtcdUp.Set(0)
            log.Printf("[ERROR] Etcd health check (via apiserver) failed: %v", err)
        } else {
            metrics.EtcdUp.Set(1)
        }
        return
    }

    // æ­¥éª¤ 3: æ£€æŸ¥ Pod è¿è¡ŒçŠ¶æ€
    hasRunningPod := false
    for _, pod := range pods.Items {
        if pod.Status.Phase == corev1.PodRunning {
            hasRunningPod = true
            break
        }
    }

    if hasRunningPod {
        metrics.EtcdUp.Set(1)
    } else {
        metrics.EtcdUp.Set(0)
    }
}
```

**åŸç†**ï¼ˆå…¼å®¹å¤šç§éƒ¨ç½²æ–¹å¼ï¼‰ï¼š
- **å†…éƒ¨ etcd**: æ£€æŸ¥ `kube-system` ä¸­çš„ etcd Pod çŠ¶æ€
- **å¤–éƒ¨ etcd**: é€šè¿‡ API Server çš„ `/livez` ç«¯ç‚¹é—´æ¥éªŒè¯ï¼ˆAPI Server ä¾èµ– etcdï¼‰

**æš´éœ²æŒ‡æ ‡**ï¼š
```prometheus
# HELP etcd_up Etcd cluster availability (1=up, 0=down)
# TYPE etcd_up gauge
etcd_up 1
```

---

##### 2.3.4 é›†ç¾¤å­˜å‚¨ç›‘æ§

**âœ¨ å¢å¼ºåŠŸèƒ½**ï¼šé€šè¿‡åˆ›å»ºæµ‹è¯• PVC æ¥çœŸæ­£éªŒè¯å­˜å‚¨åŠŸèƒ½ï¼ˆå·²å®ç°ï¼‰

**å®ç°ä»£ç **ï¼š
```go
func (c *KubernetesCollector) checkStorageClasses() {
    // åˆ—å‡ºæ‰€æœ‰ StorageClass
    storageClasses, err := c.clientset.StorageV1().StorageClasses().List(ctx, metav1.ListOptions{})
    if err != nil {
        log.Printf("[ERROR] Failed to list StorageClasses: %v", err)
        return
    }

    // å¹¶å‘æµ‹è¯•æ¯ä¸ªå­˜å‚¨ç±»çš„å®é™…åŠŸèƒ½
    for _, sc := range storageClasses.Items {
        go c.testStorageClass(sc.Name)
    }
}

// testStorageClass é€šè¿‡åˆ›å»ºæµ‹è¯• PVC æ¥çœŸæ­£éªŒè¯å­˜å‚¨ç±»åŠŸèƒ½
func (c *KubernetesCollector) testStorageClass(storageClassName string) {
    // 1. åˆ›å»ºå”¯ä¸€çš„æµ‹è¯• PVCï¼ˆ1Mi æœ€å°å®¹é‡ï¼‰
    testPVCName := fmt.Sprintf("health-check-test-%s-%d", storageClassName, time.Now().Unix())

    pvc := &corev1.PersistentVolumeClaim{
        ObjectMeta: metav1.ObjectMeta{
            Name:      testPVCName,
            Namespace: "rbd-system",
            Labels:    map[string]string{"app": "health-console", "purpose": "storage-test"},
        },
        Spec: corev1.PersistentVolumeClaimSpec{
            StorageClassName: &storageClassName,
            AccessModes:      []corev1.PersistentVolumeAccessMode{corev1.ReadWriteOnce},
            Resources: corev1.ResourceRequirements{
                Requests: corev1.ResourceList{
                    corev1.ResourceStorage: resource.MustParse("1Mi"),
                },
            },
        },
    }

    // 2. åˆ›å»ºæµ‹è¯• PVC
    c.clientset.CoreV1().PersistentVolumeClaims("rbd-system").Create(ctx, pvc, metav1.CreateOptions{})

    // 3. ç¡®ä¿æœ€ååˆ é™¤ PVC
    defer func() {
        c.clientset.CoreV1().PersistentVolumeClaims("rbd-system").Delete(ctx, testPVCName, metav1.DeleteOptions{})
    }()

    // 4. ç­‰å¾… PVC ç»‘å®šï¼ˆ30 ç§’è¶…æ—¶ï¼‰
    timeout := time.After(30 * time.Second)
    ticker := time.NewTicker(2 * time.Second)
    defer ticker.Stop()

    for {
        select {
        case <-timeout:
            metrics.ClusterStorageUp.WithLabelValues(storageClassName).Set(0)
            return
        case <-ticker.C:
            currentPVC, _ := c.clientset.CoreV1().PersistentVolumeClaims("rbd-system").Get(ctx, testPVCName, metav1.GetOptions{})
            if currentPVC.Status.Phase == corev1.ClaimBound {
                metrics.ClusterStorageUp.WithLabelValues(storageClassName).Set(1)
                return
            }
            if currentPVC.Status.Phase == corev1.ClaimLost {
                metrics.ClusterStorageUp.WithLabelValues(storageClassName).Set(0)
                return
            }
        }
    }
}
```

**åŸç†**ï¼ˆçœŸå®åŠŸèƒ½éªŒè¯ï¼‰ï¼š
1. **åˆ—å‡ºå­˜å‚¨ç±»**ï¼šè·å–é›†ç¾¤ä¸­æ‰€æœ‰ StorageClass
2. **å¹¶å‘æµ‹è¯•**ï¼šä¸ºæ¯ä¸ªå­˜å‚¨ç±»å¯åŠ¨ç‹¬ç«‹ goroutine è¿›è¡Œæµ‹è¯•
3. **åˆ›å»ºæµ‹è¯• PVC**ï¼š
   - å¤§å°ï¼š1Miï¼ˆæœ€å°å®¹é‡ï¼Œé™ä½å¼€é”€ï¼‰
   - å‘½åï¼š`health-check-test-{storageClassName}-{timestamp}`ï¼ˆå”¯ä¸€æ€§ï¼‰
   - å‘½åç©ºé—´ï¼š`rbd-system`ï¼ˆå¥åº·æ£€æŸ¥ä¸“ç”¨ï¼‰
   - æ ‡ç­¾ï¼š`app=health-console, purpose=storage-test`ï¼ˆä¾¿äºè¯†åˆ«å’Œæ¸…ç†ï¼‰
4. **ç­‰å¾…ç»‘å®š**ï¼šæ¯ 2 ç§’æ£€æŸ¥ä¸€æ¬¡ PVC çŠ¶æ€ï¼Œæœ€é•¿ç­‰å¾… 30 ç§’
5. **çŠ¶æ€åˆ¤æ–­**ï¼š
   - `Bound` â†’ å­˜å‚¨ç±»å¯ç”¨ï¼Œè®¾ç½®æŒ‡æ ‡ä¸º 1 âœ…
   - `Lost` â†’ å­˜å‚¨ç±»æ•…éšœï¼Œè®¾ç½®æŒ‡æ ‡ä¸º 0 âŒ
   - è¶…æ—¶ï¼ˆ30ç§’æœªç»‘å®šï¼‰â†’ å­˜å‚¨ç±»ä¸å¯ç”¨ï¼Œè®¾ç½®æŒ‡æ ‡ä¸º 0 âŒ
6. **è‡ªåŠ¨æ¸…ç†**ï¼šé€šè¿‡ defer ç¡®ä¿æµ‹è¯• PVC å’ŒåŠ¨æ€ä¾›åº”çš„ PV éƒ½è¢«åˆ é™¤

**æš´éœ²æŒ‡æ ‡**ï¼š
```prometheus
# HELP cluster_storage_up Storage class availability (1=up, 0=down)
# TYPE cluster_storage_up gauge
cluster_storage_up{storage_class="local-path"} 1
cluster_storage_up{storage_class="nfs-client"} 1
cluster_storage_up{storage_class="longhorn"} 0  # æ•…éšœç¤ºä¾‹
```

**ä¼˜åŠ¿**ï¼š
- âœ… **çœŸå®éªŒè¯**ï¼šä¸ä»…æ£€æŸ¥å¯¹è±¡å­˜åœ¨ï¼Œè¿˜éªŒè¯å­˜å‚¨ä¾›åº”èƒ½åŠ›
- âœ… **æ•…éšœå‘ç°**ï¼šèƒ½æ£€æµ‹å‡º NFS æœåŠ¡å™¨å®•æœºã€CSI é©±åŠ¨å¼‚å¸¸ã€é…é¢ä¸è¶³ç­‰é—®é¢˜
- âœ… **è‡ªåŠ¨æ¸…ç†**ï¼šæµ‹è¯•å®Œæˆåè‡ªåŠ¨åˆ é™¤ PVC å’Œ PVï¼Œæ— æ®‹ç•™
- âœ… **æœ€å°å¼€é”€**ï¼šä»… 1Mi å®¹é‡ï¼Œæµ‹è¯•æ—¶é—´ < 30 ç§’
- âœ… **å¹¶å‘é«˜æ•ˆ**ï¼šå¤šä¸ªå­˜å‚¨ç±»å¹¶è¡Œæµ‹è¯•ï¼Œä¸å½±å“é‡‡é›†å‘¨æœŸ
- âœ… **å®‰å…¨å¯é **ï¼šä½¿ç”¨å”¯ä¸€å‘½åé¿å…å†²çªï¼Œdefer ä¿è¯æ¸…ç†

**é€‚ç”¨åœºæ™¯**ï¼š
- åŠ¨æ€ä¾›åº”å­˜å‚¨ç±»ï¼ˆLocal Pathã€NFSã€Longhornã€Ceph RBD ç­‰ï¼‰
- æ”¯æŒå¿«é€Ÿä¾›åº”çš„å­˜å‚¨åç«¯ï¼ˆå»ºè®® < 30 ç§’ï¼‰

**æ³¨æ„äº‹é¡¹**ï¼š
- å¯¹äºæ…¢é€Ÿå­˜å‚¨åç«¯ï¼ˆå¦‚ç½‘ç»œå­˜å‚¨ï¼‰ï¼Œå¯èƒ½ä¼šè¶…æ—¶è¯¯æŠ¥
- æµ‹è¯• PVC ä¼šçŸ­æš‚å ç”¨å‘½åç©ºé—´èµ„æºé…é¢ï¼ˆé€šå¸¸ < 30 ç§’ï¼‰
- ä¸æ”¯æŒé™æ€ä¾›åº”çš„å­˜å‚¨ç±»ä¼šä¸€ç›´ Pending ç›´åˆ°è¶…æ—¶

---

### 3. å®¹å™¨é•œåƒä»“åº“ç›‘æ§

#### 3.1 è§„åˆ™å®šä¹‰

| ç›‘æ§æŒ‡æ ‡ | å‘Šè­¦æ¡ä»¶ | è¯´æ˜ |
|---------|---------|------|
| å†…éƒ¨ Registry è¿æ¥ | è¿æ¥å¤±è´¥ | é•œåƒä»“åº“ä¸å¯è¾¾ |
| å¤–éƒ¨ Registry è¿æ¥ | è¿æ¥å¤±è´¥ | é•œåƒä»“åº“ä¸å¯è¾¾ |

#### 3.2 å®ç°çŠ¶æ€

âœ… **å·²å®Œæ•´å®ç°**

**å®ç°æ–‡ä»¶**: `collectors/registry.go:1`

#### 3.3 å®ç°åŸç†

```go
type RegistryCollector struct {
    registries []config.RegistryConfig  // æ”¯æŒå¤šä¸ªé•œåƒä»“åº“
    interval   time.Duration
    ctx        context.Context
    cancel     context.CancelFunc
}
```

**å·¥ä½œæµç¨‹**ï¼š

1. **é…ç½®åŠ è½½**: æ”¯æŒå¤šä¸ªé•œåƒä»“åº“å®ä¾‹
   ```bash
   REGISTRY_1_NAME=internal
   REGISTRY_1_URL=registry.cluster.local
   REGISTRY_1_USER=admin
   REGISTRY_1_PASSWORD=password
   REGISTRY_1_INSECURE=false

   REGISTRY_2_NAME=external
   REGISTRY_2_URL=harbor.example.com
   ...
   ```

2. **å¥åº·æ£€æŸ¥**: ä½¿ç”¨ Docker Registry API v2 è§„èŒƒ
   ```go
   func (c *RegistryCollector) checkRegistry(regConfig config.RegistryConfig) {
       // è§„èŒƒåŒ– URL
       url := regConfig.URL
       if !strings.HasPrefix(url, "http://") && !strings.HasPrefix(url, "https://") {
           if regConfig.Insecure {
               url = "http://" + url
           } else {
               url = "https://" + url
           }
       }
       url += "/v2/"  // Docker Registry API v2 ç«¯ç‚¹

       // åˆ›å»º HTTP å®¢æˆ·ç«¯
       client := &http.Client{
           Timeout: 10 * time.Second,
       }

       // æ”¯æŒä¸å®‰å…¨çš„ TLSï¼ˆç”¨äºè‡ªç­¾åè¯ä¹¦ï¼‰
       if regConfig.Insecure {
           client.Transport = &http.Transport{
               TLSClientConfig: &tls.Config{InsecureSkipVerify: true},
           }
       }

       // åˆ›å»ºè¯·æ±‚
       req, _ := http.NewRequest("GET", url, nil)

       // æ”¯æŒ Basic Auth
       if regConfig.Username != "" && regConfig.Password != "" {
           req.SetBasicAuth(regConfig.Username, regConfig.Password)
       }

       // å‘é€è¯·æ±‚
       resp, err := client.Do(req)
       if err != nil {
           metrics.RegistryUp.WithLabelValues(regConfig.Name).Set(0)
           return
       }
       defer resp.Body.Close()

       // Docker Registry API v2 è¿”å› 200 æˆ– 401 éƒ½è¡¨ç¤ºæœåŠ¡æ­£å¸¸
       // 401 è¡¨ç¤ºéœ€è¦è®¤è¯ä½†æœåŠ¡å¯ç”¨
       if resp.StatusCode == http.StatusOK || resp.StatusCode == http.StatusUnauthorized {
           metrics.RegistryUp.WithLabelValues(regConfig.Name).Set(1)
       } else {
           metrics.RegistryUp.WithLabelValues(regConfig.Name).Set(0)
       }
   }
   ```

**å…³é”®è®¾è®¡**ï¼š
- ğŸŒ æ”¯æŒ HTTP å’Œ HTTPS
- ğŸ” æ”¯æŒ Basic Auth è®¤è¯
- ğŸ”“ æ”¯æŒä¸å®‰å…¨æ¨¡å¼ï¼ˆè‡ªç­¾åè¯ä¹¦ï¼‰
- ğŸ“ éµå¾ª Docker Registry API v2 è§„èŒƒ
- âœ… å°† 401 è§†ä¸ºæ­£å¸¸ï¼ˆè®¤è¯é”™è¯¯ä½†æœåŠ¡å¯ç”¨ï¼‰

**æš´éœ²æŒ‡æ ‡**ï¼š
```prometheus
# HELP registry_up Container registry availability (1=up, 0=down)
# TYPE registry_up gauge
registry_up{instance="internal"} 1
registry_up{instance="external"} 1
```

---

### 4. å¯¹è±¡å­˜å‚¨ç›‘æ§

#### 4.1 è§„åˆ™å®šä¹‰

| ç›‘æ§æŒ‡æ ‡ | å‘Šè­¦æ¡ä»¶ | è¯´æ˜ |
|---------|---------|------|
| å­˜å‚¨æœåŠ¡ | minio_up == 0 | å¯¹è±¡å­˜å‚¨ä¸å¯ç”¨ |

#### 4.2 å®ç°çŠ¶æ€

âœ… **å·²å®Œæ•´å®ç°**

**å®ç°æ–‡ä»¶**: `collectors/storage.go:1`

#### 4.3 å®ç°åŸç†

```go
type StorageCollector struct {
    minioConfig config.MinIOConfig
    interval    time.Duration
    ctx         context.Context
    cancel      context.CancelFunc
}
```

**å·¥ä½œæµç¨‹**ï¼š

1. **é…ç½®åŠ è½½**:
   ```bash
   MINIO_ENDPOINT=minio.storage.svc.cluster.local:9000
   MINIO_ACCESS_KEY=minioadmin
   MINIO_SECRET_KEY=minioadmin
   MINIO_USE_SSL=false
   ```

2. **è¿æ¥æ£€æµ‹**:
   ```go
   func (c *StorageCollector) checkMinIO() {
       // åˆ›å»º MinIO å®¢æˆ·ç«¯
       minioClient, err := minio.New(c.minioConfig.Endpoint, &minio.Options{
           Creds:  credentials.NewStaticV4(
               c.minioConfig.AccessKey,
               c.minioConfig.SecretKey,
               "",
           ),
           Secure: c.minioConfig.UseSSL,
       })
       if err != nil {
           metrics.MinIOUp.Set(0)
           return
       }

       // é€šè¿‡ ListBuckets æ“ä½œæ£€æµ‹æœåŠ¡å¯ç”¨æ€§
       ctx, cancel := context.WithTimeout(context.Background(), 10*time.Second)
       defer cancel()

       _, err = minioClient.ListBuckets(ctx)
       if err != nil {
           metrics.MinIOUp.Set(0)
           log.Printf("[ERROR] MinIO health check failed: %v", err)
       } else {
           metrics.MinIOUp.Set(1)
       }
   }
   ```

**åŸç†**ï¼š
- ä½¿ç”¨ MinIO Go SDK å®˜æ–¹åº“
- é€šè¿‡ `ListBuckets()` æ“ä½œéªŒè¯æœåŠ¡å¯ç”¨æ€§
- æ”¯æŒ S3 å…¼å®¹çš„å¯¹è±¡å­˜å‚¨ï¼ˆAWS S3ã€MinIOã€é˜¿é‡Œäº‘ OSS ç­‰ï¼‰
- å¦‚æœæœªé…ç½® `MINIO_ENDPOINT`ï¼Œè‡ªåŠ¨è·³è¿‡ç›‘æ§

**æš´éœ²æŒ‡æ ‡**ï¼š
```prometheus
# HELP minio_up MinIO/S3 object storage availability (1=up, 0=down)
# TYPE minio_up gauge
minio_up 1
```

---

## å››ã€æŠ€æœ¯æ¶æ„æ€»ç»“

### 4.1 æ ¸å¿ƒè®¾è®¡æ¨¡å¼

#### 4.1.1 é‡‡é›†å™¨æ¥å£

æ‰€æœ‰é‡‡é›†å™¨éƒ½å®ç°ç»Ÿä¸€æ¥å£ï¼š
```go
type Collector interface {
    Start()  // å¯åŠ¨é‡‡é›†å™¨
    Stop()   // åœæ­¢é‡‡é›†å™¨
}
```

**å®ç°ç¤ºä¾‹**ï¼š
```go
type DatabaseCollector struct {
    databases []config.DatabaseConfig
    interval  time.Duration
    ctx       context.Context
    cancel    context.CancelFunc
}

func (c *DatabaseCollector) Start() {
    go func() {
        ticker := time.NewTicker(c.interval)
        defer ticker.Stop()

        for {
            select {
            case <-ticker.C:
                c.collect()  // æ‰§è¡Œé‡‡é›†
            case <-c.ctx.Done():
                return  // ä¼˜é›…é€€å‡º
            }
        }
    }()
}

func (c *DatabaseCollector) Stop() {
    c.cancel()  // å‘é€åœæ­¢ä¿¡å·
}
```

**ä¼˜åŠ¿**ï¼š
- ç»Ÿä¸€çš„ç”Ÿå‘½å‘¨æœŸç®¡ç†
- æ”¯æŒä¼˜é›…å…³é—­
- éé˜»å¡å¯åŠ¨

---

#### 4.1.2 æŒ‡æ ‡æ³¨å†Œæœºåˆ¶

ä½¿ç”¨ Prometheus `promauto` åŒ…è‡ªåŠ¨æ³¨å†ŒæŒ‡æ ‡ï¼š
```go
// metrics/metrics.go

var (
    MySQLUp = promauto.NewGaugeVec(
        prometheus.GaugeOpts{
            Name: "mysql_up",
            Help: "MySQL database availability (1=up, 0=down)",
        },
        []string{"instance"},
    )

    ClusterCPUAvailablePercent = promauto.NewGauge(
        prometheus.GaugeOpts{
            Name: "cluster_cpu_available_percent",
            Help: "Cluster CPU available percentage",
        },
    )

    // ... æ›´å¤šæŒ‡æ ‡å®šä¹‰
)
```

**ä¼˜åŠ¿**ï¼š
- è‡ªåŠ¨æ³¨å†Œåˆ°é»˜è®¤ Registry
- ç±»å‹å®‰å…¨
- é›†ä¸­ç®¡ç†

---

#### 4.1.3 é…ç½®ç®¡ç†

**é…ç½®åŠ è½½æµç¨‹**ï¼š
```go
// config/config.go

type Config struct {
    MetricsPort         int
    CollectInterval     time.Duration
    Databases           []DatabaseConfig
    Registries          []RegistryConfig
    MinIO               MinIOConfig
    GRDataPath          string
    NodeCPUThreshold    float64
    NodeMemoryThreshold float64
    InCluster           bool
}

func LoadConfig() *Config {
    return &Config{
        MetricsPort:         getEnvAsInt("METRICS_PORT", 9090),
        CollectInterval:     getEnvAsDuration("COLLECT_INTERVAL", 30*time.Second),
        Databases:           loadDatabaseConfigs(),
        Registries:          loadRegistryConfigs(),
        MinIO:               loadMinIOConfig(),
        GRDataPath:          getEnv("GRDATA_PATH", "/grdata"),
        NodeCPUThreshold:    getEnvAsFloat("NODE_CPU_THRESHOLD", 80.0),
        NodeMemoryThreshold: getEnvAsFloat("NODE_MEMORY_THRESHOLD", 80.0),
        InCluster:           getEnvAsBool("IN_CLUSTER", true),
    }
}
```

**è¾…åŠ©å‡½æ•°**ï¼š
```go
func getEnv(key, defaultValue string) string {
    if value := os.Getenv(key); value != "" {
        return value
    }
    return defaultValue
}

func getEnvAsInt(key string, defaultValue int) int {
    if value := os.Getenv(key); value != "" {
        if intValue, err := strconv.Atoi(value); err == nil {
            return intValue
        }
    }
    return defaultValue
}

func getEnvAsFloat(key string, defaultValue float64) float64 {
    if value := os.Getenv(key); value != "" {
        if floatValue, err := strconv.ParseFloat(value, 64); err == nil {
            return floatValue
        }
    }
    return defaultValue
}

func getEnvAsBool(key string, defaultValue bool) bool {
    if value := os.Getenv(key); value != "" {
        if boolValue, err := strconv.ParseBool(value); err == nil {
            return boolValue
        }
    }
    return defaultValue
}

func getEnvAsDuration(key string, defaultValue time.Duration) time.Duration {
    if value := os.Getenv(key); value != "" {
        if duration, err := time.ParseDuration(value); err == nil {
            return duration
        }
    }
    return defaultValue
}
```

---

#### 4.1.4 Kubernetes å®¢æˆ·ç«¯åˆå§‹åŒ–

```go
func createKubernetesClient(inCluster bool) (*kubernetes.Clientset, error) {
    var config *rest.Config
    var err error

    if inCluster {
        // é›†ç¾¤å†…æ¨¡å¼ï¼ˆä½¿ç”¨ ServiceAccountï¼‰
        config, err = rest.InClusterConfig()
    } else {
        // é›†ç¾¤å¤–æ¨¡å¼ï¼ˆä½¿ç”¨ kubeconfigï¼‰
        kubeconfig := filepath.Join(homedir.HomeDir(), ".kube", "config")
        config, err = clientcmd.BuildConfigFromFlags("", kubeconfig)
    }

    if err != nil {
        return nil, err
    }

    return kubernetes.NewForConfig(config)
}
```

---

### 4.2 å®Œæ•´æŒ‡æ ‡åˆ—è¡¨

| æŒ‡æ ‡åç§° | ç±»å‹ | æ ‡ç­¾ | ä¼˜å…ˆçº§ | è¯´æ˜ |
|---------|------|-----|-------|------|
| `mysql_up` | Gauge | instance | P0 | MySQL å¯ç”¨æ€§ |
| `kubernetes_apiserver_up` | Gauge | - | P0 | API Server å¯ç”¨æ€§ |
| `coredns_up` | Gauge | - | P0 | CoreDNS å¯ç”¨æ€§ |
| `etcd_up` | Gauge | - | P0 | Etcd å¯ç”¨æ€§ |
| `cluster_storage_up` | Gauge | storage_class | P0 | å­˜å‚¨ç±»å¯ç”¨æ€§ |
| `registry_up` | Gauge | instance | P0 | é•œåƒä»“åº“å¯ç”¨æ€§ |
| `minio_up` | Gauge | - | P0 | MinIO å¯ç”¨æ€§ |
| `disk_space_total_bytes` | Gauge | path, node | P1 | ç£ç›˜æ€»å®¹é‡ |
| `disk_space_available_bytes` | Gauge | path, node | P1 | ç£ç›˜å¯ç”¨ç©ºé—´ |
| `disk_space_usage_percent` | Gauge | path, node | P1 | ç£ç›˜ä½¿ç”¨ç‡ |
| `cluster_cpu_total_cores` | Gauge | - | P1 | é›†ç¾¤ CPU æ€»æ ¸å¿ƒæ•° |
| `cluster_cpu_available_cores` | Gauge | - | P1 | é›†ç¾¤ CPU å¯ç”¨æ ¸å¿ƒæ•° |
| `cluster_cpu_available_percent` | Gauge | - | P1 | é›†ç¾¤ CPU å¯ç”¨ç‡ |
| `cluster_memory_total_bytes` | Gauge | - | P1 | é›†ç¾¤å†…å­˜æ€»é‡ |
| `cluster_memory_available_bytes` | Gauge | - | P1 | é›†ç¾¤å†…å­˜å¯ç”¨é‡ |
| `cluster_memory_available_percent` | Gauge | - | P1 | é›†ç¾¤å†…å­˜å¯ç”¨ç‡ |
| `node_ready` | Gauge | node | P1 | èŠ‚ç‚¹å°±ç»ªçŠ¶æ€ |
| `node_cpu_usage_percent` | Gauge | node | P1 | èŠ‚ç‚¹ CPU ä½¿ç”¨ç‡ |
| `node_memory_usage_percent` | Gauge | node | P1 | èŠ‚ç‚¹å†…å­˜ä½¿ç”¨ç‡ |
| `node_high_load` | Gauge | node | P1 | èŠ‚ç‚¹é«˜è´Ÿè½½æ ‡å¿— |
| `health_check_errors_total` | Counter | collector, error_type | - | å¥åº·æ£€æŸ¥é”™è¯¯æ€»æ•° |
| `health_check_duration_seconds` | Histogram | collector | - | å¥åº·æ£€æŸ¥è€—æ—¶ |

---

### 4.3 ç¨‹åºå¯åŠ¨æµç¨‹

```go
// main.go

func main() {
    log.Println("Starting Rainbond Health Check Console...")

    // 1. åŠ è½½é…ç½®
    cfg := config.LoadConfig()
    log.Printf("Configuration loaded: metrics_port=%d, collect_interval=%s",
        cfg.MetricsPort, cfg.CollectInterval)

    // 2. åˆå§‹åŒ–å¹¶å¯åŠ¨æ‰€æœ‰é‡‡é›†å™¨
    collectors := make([]Collector, 0)

    // æ•°æ®åº“ç›‘æ§
    if len(cfg.Databases) > 0 {
        dbCollector := collectors.NewDatabaseCollector(cfg)
        dbCollector.Start()
        collectors = append(collectors, dbCollector)
        log.Printf("Database collector started (monitoring %d instances)", len(cfg.Databases))
    }

    // K8s é›†ç¾¤ç›‘æ§
    k8sCollector, err := collectors.NewKubernetesCollector(cfg)
    if err != nil {
        log.Printf("[WARN] Failed to create Kubernetes collector: %v", err)
    } else {
        k8sCollector.Start()
        collectors = append(collectors, k8sCollector)
        log.Println("Kubernetes collector started")
    }

    // é•œåƒä»“åº“ç›‘æ§
    if len(cfg.Registries) > 0 {
        registryCollector := collectors.NewRegistryCollector(cfg)
        registryCollector.Start()
        collectors = append(collectors, registryCollector)
        log.Printf("Registry collector started (monitoring %d instances)", len(cfg.Registries))
    }

    // å¯¹è±¡å­˜å‚¨ç›‘æ§
    if cfg.MinIO.Endpoint != "" {
        storageCollector := collectors.NewStorageCollector(cfg)
        storageCollector.Start()
        collectors = append(collectors, storageCollector)
        log.Println("Storage collector started")
    }

    // ç£ç›˜ç›‘æ§
    diskCollector, err := collectors.NewDiskCollector(cfg)
    if err != nil {
        log.Printf("[WARN] Failed to create disk collector: %v", err)
    } else {
        diskCollector.Start()
        collectors = append(collectors, diskCollector)
        log.Println("Disk collector started")
    }

    // èµ„æºç›‘æ§
    resourceCollector, err := collectors.NewResourceCollector(cfg)
    if err != nil {
        log.Printf("[WARN] Failed to create resource collector: %v", err)
    } else {
        resourceCollector.Start()
        collectors = append(collectors, resourceCollector)
        log.Println("Resource collector started")
    }

    // èŠ‚ç‚¹ç›‘æ§
    nodeCollector, err := collectors.NewNodeCollector(cfg)
    if err != nil {
        log.Printf("[WARN] Failed to create node collector: %v", err)
    } else {
        nodeCollector.Start()
        collectors = append(collectors, nodeCollector)
        log.Println("Node collector started")
    }

    // 3. å¯åŠ¨ HTTP æœåŠ¡
    http.Handle("/metrics", promhttp.Handler())
    http.HandleFunc("/health", healthHandler)
    http.HandleFunc("/", indexHandler)

    addr := fmt.Sprintf(":%d", cfg.MetricsPort)
    server := &http.Server{Addr: addr}

    go func() {
        log.Printf("Metrics server listening on %s", addr)
        if err := server.ListenAndServe(); err != nil && err != http.ErrServerClosed {
            log.Fatalf("HTTP server error: %v", err)
        }
    }()

    // 4. ç­‰å¾…é€€å‡ºä¿¡å·
    sigChan := make(chan os.Signal, 1)
    signal.Notify(sigChan, os.Interrupt, syscall.SIGTERM)
    <-sigChan

    log.Println("Received shutdown signal, stopping collectors...")

    // 5. ä¼˜é›…å…³é—­
    for _, collector := range collectors {
        collector.Stop()
    }

    ctx, cancel := context.WithTimeout(context.Background(), 5*time.Second)
    defer cancel()
    if err := server.Shutdown(ctx); err != nil {
        log.Printf("HTTP server shutdown error: %v", err)
    }

    log.Println("Shutdown complete")
}
```

**HTTP ç«¯ç‚¹**ï¼š
```go
func indexHandler(w http.ResponseWriter, r *http.Request) {
    html := `
    <html>
    <head><title>Rainbond Health Check Console</title></head>
    <body>
    <h1>Rainbond Health Check Console</h1>
    <ul>
        <li><a href="/metrics">Metrics</a></li>
        <li><a href="/health">Health Check</a></li>
    </ul>
    </body>
    </html>
    `
    w.Header().Set("Content-Type", "text/html")
    w.Write([]byte(html))
}

func healthHandler(w http.ResponseWriter, r *http.Request) {
    w.Header().Set("Content-Type", "application/json")
    w.WriteHeader(http.StatusOK)
    w.Write([]byte(`{"status":"ok"}`))
}
```

---

## äº”ã€é…ç½®æŒ‡å—

### 5.1 å®Œæ•´é…ç½®ç¤ºä¾‹

```bash
# åŸºç¡€é…ç½®
METRICS_PORT=9090
COLLECT_INTERVAL=30s
IN_CLUSTER=true

# æ•°æ®åº“é…ç½®ï¼ˆæ”¯æŒå¤šå®ä¾‹ï¼‰
DB_1_NAME=internal
DB_1_HOST=mysql.database.svc.cluster.local
DB_1_PORT=3306
DB_1_USER=root
DB_1_PASSWORD=password
DB_1_DATABASE=mysql

DB_2_NAME=external
DB_2_HOST=external-mysql.example.com
DB_2_PORT=3306
DB_2_USER=rainbond
DB_2_PASSWORD=secret
DB_2_DATABASE=rainbond

# é•œåƒä»“åº“é…ç½®ï¼ˆæ”¯æŒå¤šå®ä¾‹ï¼‰
REGISTRY_1_NAME=internal
REGISTRY_1_URL=registry.cluster.local
REGISTRY_1_USER=admin
REGISTRY_1_PASSWORD=password
REGISTRY_1_INSECURE=false

REGISTRY_2_NAME=harbor
REGISTRY_2_URL=harbor.example.com
REGISTRY_2_USER=admin
REGISTRY_2_PASSWORD=Harbor12345
REGISTRY_2_INSECURE=true

# MinIO/S3 é…ç½®
MINIO_ENDPOINT=minio.storage.svc.cluster.local:9000
MINIO_ACCESS_KEY=minioadmin
MINIO_SECRET_KEY=minioadmin
MINIO_USE_SSL=false

# ç£ç›˜ç›‘æ§é…ç½®
GRDATA_PATH=/grdata

# èŠ‚ç‚¹è´Ÿè½½é˜ˆå€¼é…ç½®
NODE_CPU_THRESHOLD=80.0
NODE_MEMORY_THRESHOLD=80.0
```

### 5.2 Kubernetes éƒ¨ç½²é…ç½®

```yaml
# deploy/kubernetes/deploy.yaml

apiVersion: v1
kind: ServiceAccount
metadata:
  name: rainbond-health-console
  namespace: rbd-system
---
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRole
metadata:
  name: rainbond-health-console
rules:
- apiGroups: [""]
  resources:
    - nodes
    - pods
    - services
    - endpoints
    - persistentvolumeclaims
  verbs: ["get", "list", "watch"]
- apiGroups: ["storage.k8s.io"]
  resources:
    - storageclasses
  verbs: ["get", "list"]
- apiGroups: ["metrics.k8s.io"]
  resources:
    - nodes
    - pods
  verbs: ["get", "list"]
---
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRoleBinding
metadata:
  name: rainbond-health-console
roleRef:
  apiGroup: rbac.authorization.k8s.io
  kind: ClusterRole
  name: rainbond-health-console
subjects:
- kind: ServiceAccount
  name: rainbond-health-console
  namespace: rbd-system
---
apiVersion: v1
kind: ConfigMap
metadata:
  name: rainbond-health-console-config
  namespace: rbd-system
data:
  METRICS_PORT: "9090"
  COLLECT_INTERVAL: "30s"
  IN_CLUSTER: "true"
  GRDATA_PATH: "/grdata"
  NODE_CPU_THRESHOLD: "80.0"
  NODE_MEMORY_THRESHOLD: "80.0"
---
apiVersion: v1
kind: Secret
metadata:
  name: rainbond-health-console-secret
  namespace: rbd-system
type: Opaque
stringData:
  # æ•°æ®åº“å‡­æ®
  DB_1_NAME: "internal"
  DB_1_HOST: "mysql.database.svc.cluster.local"
  DB_1_PORT: "3306"
  DB_1_USER: "root"
  DB_1_PASSWORD: "password"

  # é•œåƒä»“åº“å‡­æ®
  REGISTRY_1_NAME: "internal"
  REGISTRY_1_URL: "registry.cluster.local"
  REGISTRY_1_USER: "admin"
  REGISTRY_1_PASSWORD: "password"

  # MinIO å‡­æ®
  MINIO_ENDPOINT: "minio.storage.svc.cluster.local:9000"
  MINIO_ACCESS_KEY: "minioadmin"
  MINIO_SECRET_KEY: "minioadmin"
---
apiVersion: apps/v1
kind: Deployment
metadata:
  name: rainbond-health-console
  namespace: rbd-system
  labels:
    app: rainbond-health-console
spec:
  replicas: 1
  selector:
    matchLabels:
      app: rainbond-health-console
  template:
    metadata:
      labels:
        app: rainbond-health-console
    spec:
      serviceAccountName: rainbond-health-console
      containers:
      - name: health-console
        image: rainbond/rainbond-health-console:latest
        imagePullPolicy: Always
        ports:
        - containerPort: 9090
          name: metrics
          protocol: TCP
        envFrom:
        - configMapRef:
            name: rainbond-health-console-config
        - secretRef:
            name: rainbond-health-console-secret
        volumeMounts:
        - name: grdata
          mountPath: /grdata
          readOnly: true
        resources:
          requests:
            cpu: 100m
            memory: 128Mi
          limits:
            cpu: 500m
            memory: 512Mi
        livenessProbe:
          httpGet:
            path: /health
            port: 9090
          initialDelaySeconds: 10
          periodSeconds: 30
        readinessProbe:
          httpGet:
            path: /health
            port: 9090
          initialDelaySeconds: 5
          periodSeconds: 10
      volumes:
      - name: grdata
        hostPath:
          path: /grdata
          type: DirectoryOrCreate
---
apiVersion: v1
kind: Service
metadata:
  name: rainbond-health-console
  namespace: rbd-system
  labels:
    app: rainbond-health-console
spec:
  type: ClusterIP
  ports:
  - port: 9090
    targetPort: 9090
    protocol: TCP
    name: metrics
  selector:
    app: rainbond-health-console
---
apiVersion: monitoring.coreos.com/v1
kind: ServiceMonitor
metadata:
  name: rainbond-health-console
  namespace: rbd-system
  labels:
    app: rainbond-health-console
spec:
  selector:
    matchLabels:
      app: rainbond-health-console
  endpoints:
  - port: metrics
    interval: 30s
    path: /metrics
```

---

## å…­ã€å‘Šè­¦è§„åˆ™ç¤ºä¾‹

### 6.1 Prometheus å‘Šè­¦è§„åˆ™

```yaml
# deploy/kubernetes/prometheus-rules.yaml

apiVersion: monitoring.coreos.com/v1
kind: PrometheusRule
metadata:
  name: rainbond-platform-health-rules
  namespace: rbd-system
  labels:
    prometheus: kube-prometheus
spec:
  groups:

  # ========== P0 çº§åˆ«å‘Šè­¦ï¼ˆè‡´å‘½ï¼‰ ==========
  - name: rainbond-p0-critical
    interval: 30s
    rules:

    # æ•°æ®åº“ä¸å¯è¾¾
    - alert: RainbondDatabaseDown
      expr: mysql_up == 0
      for: 1m
      labels:
        severity: critical
        priority: P0
      annotations:
        summary: "Rainbond æ•°æ®åº“ä¸å¯è¾¾"
        description: "æ•°æ®åº“å®ä¾‹ {{ $labels.instance }} å·²ä¸å¯è¾¾è¶…è¿‡ 1 åˆ†é’Ÿã€‚\nå½“å‰çŠ¶æ€: {{ $value }}"

    # Kubernetes API Server ä¸å¯ç”¨
    - alert: RainbondKubernetesAPIDown
      expr: kubernetes_apiserver_up == 0
      for: 1m
      labels:
        severity: critical
        priority: P0
      annotations:
        summary: "Kubernetes API Server ä¸å¯ç”¨"
        description: "Kubernetes API Server ä¸å¯è¾¾ï¼Œå¹³å°æ ¸å¿ƒåŠŸèƒ½å°†ä¸å¯ç”¨ã€‚"

    # CoreDNS ä¸å¯ç”¨
    - alert: RainbondCoreDNSDown
      expr: coredns_up == 0
      for: 2m
      labels:
        severity: critical
        priority: P0
      annotations:
        summary: "CoreDNS æœåŠ¡ä¸å¯ç”¨"
        description: "é›†ç¾¤å†…éƒ¨ DNS è§£æå¤±è´¥ï¼Œå¯èƒ½å¯¼è‡´æœåŠ¡é—´é€šä¿¡å¼‚å¸¸ã€‚"

    # Etcd ä¸å¯ç”¨
    - alert: RainbondEtcdDown
      expr: etcd_up == 0
      for: 1m
      labels:
        severity: critical
        priority: P0
      annotations:
        summary: "Etcd é›†ç¾¤ä¸å¯ç”¨"
        description: "Kubernetes å­˜å‚¨åç«¯ Etcd ä¸å¯ç”¨ï¼Œé›†ç¾¤åŠŸèƒ½å°†å—ä¸¥é‡å½±å“ã€‚"

    # å­˜å‚¨ç±»ä¸å¯ç”¨
    - alert: RainbondStorageClassDown
      expr: cluster_storage_up == 0
      for: 5m
      labels:
        severity: critical
        priority: P0
      annotations:
        summary: "å­˜å‚¨ç±»ä¸å¯ç”¨"
        description: "å­˜å‚¨ç±» {{ $labels.storage_class }} ä¸å¯ç”¨ï¼Œæ— æ³•åˆ›å»ºæŒä¹…åŒ–å­˜å‚¨å·ã€‚"

    # é•œåƒä»“åº“ä¸å¯è¾¾
    - alert: RainbondRegistryDown
      expr: registry_up == 0
      for: 2m
      labels:
        severity: critical
        priority: P0
      annotations:
        summary: "å®¹å™¨é•œåƒä»“åº“ä¸å¯è¾¾"
        description: "é•œåƒä»“åº“ {{ $labels.instance }} ä¸å¯è¾¾ï¼Œå¯èƒ½å½±å“åº”ç”¨éƒ¨ç½²ã€‚"

    # MinIO/S3 ä¸å¯ç”¨
    - alert: RainbondMinIODown
      expr: minio_up == 0
      for: 2m
      labels:
        severity: critical
        priority: P0
      annotations:
        summary: "å¯¹è±¡å­˜å‚¨æœåŠ¡ä¸å¯ç”¨"
        description: "MinIO/S3 å¯¹è±¡å­˜å‚¨ä¸å¯ç”¨ï¼Œå¯èƒ½å½±å“æ–‡ä»¶ä¸Šä¼ å’Œå¤‡ä»½åŠŸèƒ½ã€‚"



### 6.2 å‘Šè­¦æ¥æ”¶å™¨é…ç½®ï¼ˆAlertManagerï¼‰

```yaml
# alertmanager-config.yaml

global:
  resolve_timeout: 5m

route:
  group_by: ['alertname', 'priority']
  group_wait: 10s
  group_interval: 10s
  repeat_interval: 12h
  receiver: 'default'
  routes:
  # P0 çº§åˆ«å‘Šè­¦ç«‹å³å‘é€
  - match:
      priority: P0
    receiver: 'rainbond-critical'
    group_wait: 0s
    repeat_interval: 5m
  # P1 çº§åˆ«å‘Šè­¦
  - match:
      priority: P1
    receiver: 'rainbond-high'
    repeat_interval: 30m

receivers:
- name: 'default'
  webhook_configs:
  - url: 'http://webhook-receiver:8080/alerts'

- name: 'rainbond-critical'
  webhook_configs:
  - url: 'http://webhook-receiver:8080/alerts/critical'
    send_resolved: true
  # å¯é…ç½®å…¶ä»–æ¥æ”¶å™¨ï¼ˆé‚®ä»¶ã€çŸ­ä¿¡ã€ä¼ä¸šå¾®ä¿¡ç­‰ï¼‰
  # email_configs:
  # - to: 'ops@example.com'
  #   from: 'alertmanager@example.com'
  #   smarthost: 'smtp.example.com:587'
  #   auth_username: 'alertmanager'
  #   auth_password: 'password'

- name: 'rainbond-high'
  webhook_configs:
  - url: 'http://webhook-receiver:8080/alerts/high'
    send_resolved: true
```

---

## ä¸ƒã€è¿ç»´æŒ‡å—

### 7.1 å¸¸è§é—®é¢˜æ’æŸ¥

#### é—®é¢˜ 1: èŠ‚ç‚¹åŸŸåæ— æ³•è§£æï¼ˆå‚è€ƒæ–‡æ¡£æ¡ˆä¾‹ï¼‰

**ç°è±¡**ï¼š
- Pod å†…æ— æ³•è§£æåŸŸå
- è®¿é—® ClusterIP æœåŠ¡å¤±è´¥
- CoreDNS Pod æ­£å¸¸ä½†åŠŸèƒ½å¼‚å¸¸

**æ ¹å› **ï¼š
- Flannel ç½‘ç»œåŒæ­¥å¼‚å¸¸
- watch è¿æ¥ä¸­æ–­å¯¼è‡´è·¯ç”±ä¿¡æ¯æœªåŒæ­¥

**æ’æŸ¥æ­¥éª¤**ï¼š
1. æ£€æŸ¥ CoreDNS ç›‘æ§æŒ‡æ ‡: `coredns_up`
2. æ£€æŸ¥èŠ‚ç‚¹çŠ¶æ€: `node_ready{node="node-xxx"}`
3. æŸ¥çœ‹ Flannel æ—¥å¿—: `kubectl logs -n kube-system <flannel-pod> | grep "client connection lost"`
4. æ£€æŸ¥èŠ‚ç‚¹è·¯ç”±è¡¨: `ip route show`
5. æ£€æŸ¥ VXLAN æ¥å£: `ip -d link show flannel.1`

**è§£å†³æ–¹æ¡ˆ**ï¼š
```bash
# é‡å¯ rke2-agentï¼ˆæˆ– flannelï¼‰é‡æ–°åŒæ­¥ç½‘ç»œé…ç½®
systemctl restart rke2-agent

# æˆ–é‡å¯ flannel Pod
kubectl delete pod -n kube-system <flannel-pod>
```

**ç›‘æ§æŒ‡æ ‡**ï¼š
```prometheus
# CoreDNS çŠ¶æ€
coredns_up

# èŠ‚ç‚¹å°±ç»ªçŠ¶æ€
node_ready{node="node-xxx"}
```

---

#### é—®é¢˜ 2: ç£ç›˜ç©ºé—´ä¸è¶³

**ç°è±¡**ï¼š
```prometheus
disk_space_usage_percent{path="/grdata"} > 90
```

**æ’æŸ¥æ­¥éª¤**ï¼š
1. æ£€æŸ¥ç£ç›˜ä½¿ç”¨æƒ…å†µ:
   ```bash
   df -h /grdata
   du -sh /grdata/* | sort -hr | head -20
   ```

2. æ¸…ç†æ„å»ºç¼“å­˜:
   ```bash
   # æ¸…ç† Docker ç¼“å­˜
   docker system prune -af --volumes

   # æ¸…ç†æ—§æ—¥å¿—
   find /grdata/logs -name "*.log" -mtime +7 -delete
   ```

3. æ‰©å®¹ç£ç›˜ï¼ˆå¦‚æœå¿…è¦ï¼‰

---

#### é—®é¢˜ 3: Metrics Server ä¸å¯ç”¨

**ç°è±¡**ï¼š
```
[WARN] Failed to get node metrics: the server could not find the requested resource
```

**å½±å“**ï¼š
- æ— æ³•è·å–èŠ‚ç‚¹å®é™… CPU/å†…å­˜ä½¿ç”¨ç‡
- è‡ªåŠ¨é™çº§åˆ° Pressure Conditions æ£€æµ‹

**è§£å†³æ–¹æ¡ˆ**ï¼š
```bash
# æ£€æŸ¥ Metrics Server æ˜¯å¦è¿è¡Œ
kubectl get deployment -n kube-system metrics-server

# å®‰è£… Metrics Serverï¼ˆå¦‚æœæœªå®‰è£…ï¼‰
kubectl apply -f https://github.com/kubernetes-sigs/metrics-server/releases/latest/download/components.yaml

# å¦‚æœæ˜¯è‡ªç­¾åè¯ä¹¦é—®é¢˜ï¼Œæ·»åŠ  --kubelet-insecure-tls å‚æ•°
kubectl patch deployment metrics-server -n kube-system --type='json' \
  -p='[{"op": "add", "path": "/spec/template/spec/containers/0/args/-", "value": "--kubelet-insecure-tls"}]'
```

---

### 7.2 æ€§èƒ½ä¼˜åŒ–å»ºè®®

1. **è°ƒæ•´é‡‡é›†é—´éš”**ï¼š
   ```bash
   COLLECT_INTERVAL=60s  # é™ä½é‡‡é›†é¢‘ç‡ï¼Œå‡å°‘ API è°ƒç”¨
   ```

2. **é™åˆ¶ç›‘æ§èŒƒå›´**ï¼š
   - åªç›‘æ§å…³é”®æ•°æ®åº“å®ä¾‹
   - åªç›‘æ§ç”Ÿäº§ç¯å¢ƒé•œåƒä»“åº“

3. **èµ„æºé…é¢**ï¼š
   ```yaml
   resources:
     requests:
       cpu: 100m
       memory: 128Mi
     limits:
       cpu: 500m
       memory: 512Mi
   ```

---

### 7.3 ç›‘æ§å¤§ç›˜ï¼ˆGrafanaï¼‰

å»ºè®®åˆ›å»º Grafana ä»ªè¡¨ç›˜å±•ç¤ºä»¥ä¸‹å†…å®¹ï¼š

**P0 çº§åˆ«é¢æ¿**ï¼š
- æ•°æ®åº“å¯ç”¨æ€§çŠ¶æ€çŸ©é˜µ
- K8s æ ¸å¿ƒç»„ä»¶çŠ¶æ€
- é•œåƒä»“åº“å’Œå¯¹è±¡å­˜å‚¨çŠ¶æ€

**P1 çº§åˆ«é¢æ¿**ï¼š
- /grdata ç£ç›˜ä½¿ç”¨è¶‹åŠ¿å›¾
- é›†ç¾¤èµ„æºå¯ç”¨ç‡æ—¶é—´çº¿
- èŠ‚ç‚¹çŠ¶æ€çƒ­åŠ›å›¾

**Grafana JSON æ¨¡æ¿å‚è€ƒ**ï¼ˆéƒ¨åˆ†ï¼‰ï¼š
```json
{
  "panels": [
    {
      "title": "P0 - åŸºç¡€è®¾æ–½å¯ç”¨æ€§",
      "type": "stat",
      "targets": [
        {
          "expr": "mysql_up",
          "legendFormat": "MySQL - {{ instance }}"
        },
        {
          "expr": "kubernetes_apiserver_up",
          "legendFormat": "K8s API Server"
        },
        {
          "expr": "coredns_up",
          "legendFormat": "CoreDNS"
        }
      ],
      "fieldConfig": {
        "defaults": {
          "thresholds": {
            "steps": [
              {"value": 0, "color": "red"},
              {"value": 1, "color": "green"}
            ]
          }
        }
      }
    },
    {
      "title": "é›†ç¾¤èµ„æºå¯ç”¨ç‡",
      "type": "graph",
      "targets": [
        {
          "expr": "cluster_cpu_available_percent",
          "legendFormat": "CPU å¯ç”¨ç‡"
        },
        {
          "expr": "cluster_memory_available_percent",
          "legendFormat": "å†…å­˜å¯ç”¨ç‡"
        }
      ],
      "alert": {
        "conditions": [
          {
            "evaluator": {"params": [10], "type": "lt"},
            "query": {"params": ["A", "5m", "now"]}
          }
        ]
      }
    }
  ]
}
```

---

## å…«ã€æ€»ç»“ä¸å»ºè®®

### 8.1 å®ç°å®Œæˆåº¦

âœ… **æ‰€æœ‰ç›‘æ§è§„åˆ™å·² 100% å®ç°**

| ç±»åˆ« | è§„åˆ™æ•° | å®ç°çŠ¶æ€ |
|------|-------|---------|
| P0 - æ•°æ®åº“ | 2 | âœ… å®Œæˆ |
| P0 - Kubernetes | 4 | âœ… å®Œæˆ |
| P0 - é•œåƒä»“åº“ | 2 | âœ… å®Œæˆ |
| P0 - å¯¹è±¡å­˜å‚¨ | 1 | âœ… å®Œæˆ |
| P1 - ç£ç›˜ç©ºé—´ | 3 | âœ… å®Œæˆ |
| P1 - è®¡ç®—èµ„æº | 2 | âœ… å®Œæˆ |
| P1 - èŠ‚ç‚¹çŠ¶æ€ | 2 | âœ… å®Œæˆ |
| **æ€»è®¡** | **16** | **16 âœ…** |

---

### 8.2 æ¶æ„ä¼˜åŠ¿

1. **æ¨¡å—åŒ–è®¾è®¡**ï¼šé‡‡é›†å™¨ç‹¬ç«‹å¯ç»´æŠ¤
2. **äº‘åŸç”Ÿå‹å¥½**ï¼šå®Œå…¨é€‚é… Kubernetes ç”Ÿæ€
3. **é«˜å¯ç”¨**ï¼šå•ä¸ªé‡‡é›†å™¨æ•…éšœä¸å½±å“å…¶ä»–ç›‘æ§
4. **å¯æ‰©å±•**ï¼šæ˜“äºæ·»åŠ æ–°çš„ç›‘æ§è§„åˆ™
5. **æ ‡å‡†åŒ–**ï¼šä½¿ç”¨ Prometheus æ ‡å‡†æ ¼å¼

---

### 8.3 æ”¹è¿›å»ºè®®

#### ~~å»ºè®® 1: å¢å¼ºå­˜å‚¨ç±»ç›‘æ§~~ âœ… å·²å®ç°

**å®ç°çŠ¶æ€**ï¼šâœ… **å·²åœ¨ collectors/kubernetes.go ä¸­å®ç°**

**å®ç°æ–¹å¼**ï¼šé€šè¿‡åˆ›å»ºæµ‹è¯• PVC æ¥çœŸæ­£éªŒè¯å­˜å‚¨ç±»åŠŸèƒ½

```go
// testStorageClass é€šè¿‡åˆ›å»ºæµ‹è¯• PVC æ¥éªŒè¯å­˜å‚¨ç±»æ˜¯å¦çœŸæ­£å¯ç”¨
func (c *KubernetesCollector) testStorageClass(storageClassName string) {
    // 1. åˆ›å»ºå”¯ä¸€çš„æµ‹è¯• PVCï¼ˆ1Mi æœ€å°å®¹é‡ï¼‰
    testPVCName := fmt.Sprintf("health-check-test-%s-%d", storageClassName, time.Now().Unix())

    pvc := &corev1.PersistentVolumeClaim{
        ObjectMeta: metav1.ObjectMeta{
            Name:      testPVCName,
            Namespace: "rbd-system",
            Labels: map[string]string{
                "app":     "health-console",
                "purpose": "storage-test",
            },
        },
        Spec: corev1.PersistentVolumeClaimSpec{
            StorageClassName: &storageClassName,
            AccessModes:      []corev1.PersistentVolumeAccessMode{corev1.ReadWriteOnce},
            Resources: corev1.ResourceRequirements{
                Requests: corev1.ResourceList{
                    corev1.ResourceStorage: resource.MustParse("1Mi"),
                },
            },
        },
    }

    // 2. åˆ›å»ºæµ‹è¯• PVC
    _, err := c.clientset.CoreV1().PersistentVolumeClaims("rbd-system").Create(ctx, pvc, metav1.CreateOptions{})
    if err != nil {
        metrics.ClusterStorageUp.WithLabelValues(storageClassName).Set(0)
        return
    }

    // 3. ä½¿ç”¨ defer ç¡®ä¿ PVC æœ€ç»ˆä¼šè¢«åˆ é™¤
    defer func() {
        c.clientset.CoreV1().PersistentVolumeClaims("rbd-system").Delete(ctx, testPVCName, metav1.DeleteOptions{})
        log.Printf("Cleaned up test PVC %s for storage class %s", testPVCName, storageClassName)
    }()

    // 4. ç­‰å¾… PVC ç»‘å®šï¼ˆ30 ç§’è¶…æ—¶ï¼‰
    timeout := time.After(30 * time.Second)
    ticker := time.NewTicker(2 * time.Second)
    defer ticker.Stop()

    for {
        select {
        case <-timeout:
            // è¶…æ—¶ï¼Œå­˜å‚¨ç±»å¯èƒ½ä¸å¯ç”¨
            metrics.ClusterStorageUp.WithLabelValues(storageClassName).Set(0)
            return

        case <-ticker.C:
            currentPVC, _ := c.clientset.CoreV1().PersistentVolumeClaims("rbd-system").Get(ctx, testPVCName, metav1.GetOptions{})

            if currentPVC.Status.Phase == corev1.ClaimBound {
                // PVC æˆåŠŸç»‘å®šï¼Œå­˜å‚¨ç±»å¯ç”¨
                metrics.ClusterStorageUp.WithLabelValues(storageClassName).Set(1)
                return
            }

            if currentPVC.Status.Phase == corev1.ClaimLost {
                // PVC ä¸¢å¤±ï¼Œå­˜å‚¨ç±»æœ‰é—®é¢˜
                metrics.ClusterStorageUp.WithLabelValues(storageClassName).Set(0)
                return
            }
        }
    }
}
```

**å®ç°ç‰¹ç‚¹**ï¼š
- âœ… çœŸæ­£éªŒè¯å­˜å‚¨ä¾›åº”åŠŸèƒ½ï¼ˆä¸ä»…ä»…æ£€æŸ¥å¯¹è±¡å­˜åœ¨æ€§ï¼‰
- âœ… èƒ½å¤Ÿå‘ç°å­˜å‚¨åç«¯æ•…éšœï¼ˆNFS æœåŠ¡å™¨å®•æœºã€CSI é©±åŠ¨å¼‚å¸¸ç­‰ï¼‰
- âœ… è‡ªåŠ¨æ¸…ç†ï¼šé€šè¿‡ defer ç¡®ä¿æµ‹è¯• PVC å’ŒåŠ¨æ€ä¾›åº”çš„ PV éƒ½è¢«åˆ é™¤
- âœ… æœ€å°å¼€é”€ï¼šä»…åˆ›å»º 1Mi çš„ PVCï¼Œå¹¶åœ¨éªŒè¯å®Œæˆåç«‹å³åˆ é™¤
- âœ… å¹¶å‘æµ‹è¯•ï¼šå¤šä¸ªå­˜å‚¨ç±»å¹¶å‘éªŒè¯ï¼Œæé«˜æ•ˆç‡
- âœ… è¶…æ—¶ä¿æŠ¤ï¼š30 ç§’å†…æœªç»‘å®šè§†ä¸ºä¸å¯ç”¨
- âœ… å”¯ä¸€å‘½åï¼šä½¿ç”¨æ—¶é—´æˆ³é¿å… PVC åç§°å†²çª

**å·¥ä½œæµç¨‹**ï¼š
1. ä¸ºæ¯ä¸ªå­˜å‚¨ç±»åˆ›å»ºä¸€ä¸ª 1Mi çš„æµ‹è¯• PVC
2. æ¯ 2 ç§’æ£€æŸ¥ä¸€æ¬¡ PVC çŠ¶æ€
3. å¦‚æœ PVC å˜ä¸º `Bound` çŠ¶æ€ â†’ å­˜å‚¨ç±»å¯ç”¨ âœ…
4. å¦‚æœ PVC å˜ä¸º `Lost` çŠ¶æ€ â†’ å­˜å‚¨ç±»æ•…éšœ âŒ
5. å¦‚æœ 30 ç§’å†…æœªç»‘å®š â†’ å­˜å‚¨ç±»ä¸å¯ç”¨æˆ–ä¾›åº”ç¼“æ…¢ âŒ
6. æ— è®ºæˆåŠŸæˆ–å¤±è´¥ï¼Œæœ€åéƒ½ä¼šåˆ é™¤æµ‹è¯• PVC

**ä¼˜åŠ¿**ï¼š
- çœŸæ­£éªŒè¯å­˜å‚¨åŠŸèƒ½å¯ç”¨æ€§ï¼ˆèƒ½å¤Ÿå‘ç°åº•å±‚å­˜å‚¨æ•…éšœï¼‰
- é€‚ç”¨äºæ‰€æœ‰æ”¯æŒåŠ¨æ€ä¾›åº”çš„ StorageClass
- å¯¹é›†ç¾¤å½±å“æå°ï¼ˆ1Mi æœ€å°å®¹é‡ï¼Œç«‹å³åˆ é™¤ï¼‰

**æ³¨æ„äº‹é¡¹**ï¼š
- æµ‹è¯• PVC ä¼šåœ¨ `rbd-system` å‘½åç©ºé—´ä¸­çŸ­æš‚å­˜åœ¨ï¼ˆé€šå¸¸ < 30 ç§’ï¼‰
- åŠ¨æ€ä¾›åº”çš„ PV ä¼šéš PVC åˆ é™¤è‡ªåŠ¨å›æ”¶
- å¦‚æœå­˜å‚¨ç±»ä¸æ”¯æŒåŠ¨æ€ä¾›åº”ï¼ŒPVC ä¼šä¸€ç›´ Pending ç›´åˆ°è¶…æ—¶

---

#### å»ºè®® 2: æ·»åŠ ç½‘ç»œè´¨é‡ç›‘æ§

**å‚è€ƒæ–‡æ¡£æ¡ˆä¾‹**ï¼ˆFlannel ç½‘ç»œåŒæ­¥å¼‚å¸¸ï¼‰ï¼Œå»ºè®®å¢åŠ ï¼š

```go
// collectors/network.go

type NetworkCollector struct {
    clientset *kubernetes.Clientset
    interval  time.Duration
}

func (c *NetworkCollector) checkFlannelHealth() {
    // æ£€æŸ¥ Flannel Pod çŠ¶æ€
    pods, _ := c.clientset.CoreV1().Pods("kube-system").List(ctx, metav1.ListOptions{
        LabelSelector: "app=flannel",
    })

    // æ£€æŸ¥ Pod é‡å¯æ¬¡æ•°
    for _, pod := range pods.Items {
        for _, containerStatus := range pod.Status.ContainerStatuses {
            if containerStatus.RestartCount > 5 {
                // Flannel é¢‘ç¹é‡å¯ï¼Œå¯èƒ½å­˜åœ¨ç½‘ç»œé—®é¢˜
                metrics.FlannelUnhealthy.WithLabelValues(pod.Spec.NodeName).Set(1)
            }
        }
    }
}
```

**æ–°å¢æŒ‡æ ‡**ï¼š
```prometheus
flannel_unhealthy{node="xxx"} 1  # Flannel ä¸å¥åº·
network_watch_disconnections_total{node="xxx"} 10  # watch è¿æ¥ä¸­æ–­æ¬¡æ•°
```

---

#### å»ºè®® 3: å¢åŠ å†å²æ•°æ®åˆ†æ

**å»ºè®®**ï¼šè®°å½•å†å²æ•…éšœäº‹ä»¶
```go
// metrics/metrics.go

var (
    HealthCheckFailureHistory = promauto.NewCounterVec(
        prometheus.CounterOpts{
            Name: "health_check_failure_total",
            Help: "Total number of health check failures by type",
        },
        []string{"check_type", "target"},
    )
)
```

**ç”¨é€”**ï¼š
- åˆ†ææ•…éšœé¢‘ç‡
- è¯†åˆ«ä¸ç¨³å®šçš„ç»„ä»¶
- è¶‹åŠ¿é¢„æµ‹

---

#### å»ºè®® 4: æ·»åŠ è‡ªæ„ˆèƒ½åŠ›

**ç¤ºä¾‹**ï¼šè‡ªåŠ¨é‡å¯æ•…éšœç»„ä»¶
```go
func (c *KubernetesCollector) autoHealCoreDNS() {
    if c.corednsFailureCount > 3 {
        // å°è¯•é‡å¯ CoreDNS Pod
        pods, _ := c.clientset.CoreV1().Pods("kube-system").List(ctx, metav1.ListOptions{
            LabelSelector: "k8s-app=kube-dns",
        })

        for _, pod := range pods.Items {
            c.clientset.CoreV1().Pods("kube-system").Delete(ctx, pod.Name, metav1.DeleteOptions{})
            log.Printf("[INFO] Auto-healing: Restarted CoreDNS pod %s", pod.Name)
        }
    }
}
```

**æ³¨æ„**ï¼šéœ€è¦è°¨æ…ä½¿ç”¨ï¼Œé¿å…è¯¯æ“ä½œ

---

### 8.4 æœ€ä½³å®è·µ

1. **å®šæœŸå®¡æŸ¥å‘Šè­¦è§„åˆ™**ï¼šæ ¹æ®å®é™…æƒ…å†µè°ƒæ•´é˜ˆå€¼
2. **å»ºç«‹å‘Šè­¦å‡çº§æœºåˆ¶**ï¼šP0 çº§åˆ«ç«‹å³é€šçŸ¥ï¼ŒP1 çº§åˆ«å®šæœŸæ±‡æ€»
3. **é…åˆæ—¥å¿—ç³»ç»Ÿ**ï¼šç›‘æ§æŒ‡æ ‡ + æ—¥å¿—åˆ†æ = å®Œæ•´æ•…éšœå®šä½
4. **å®šæœŸæ¼”ç»ƒ**ï¼šæ¨¡æ‹Ÿæ•…éšœåœºæ™¯ï¼ŒéªŒè¯ç›‘æ§æœ‰æ•ˆæ€§
5. **æ–‡æ¡£åŒæ­¥**ï¼šç›‘æ§è§„åˆ™å˜æ›´æ—¶åŒæ­¥æ›´æ–°æ–‡æ¡£

---

### 8.5 å…³é”®æ–‡ä»¶è·¯å¾„ç´¢å¼•

| åŠŸèƒ½æ¨¡å— | æ–‡ä»¶è·¯å¾„ |
|---------|---------|
| ç¨‹åºå…¥å£ | `main.go:1` |
| é…ç½®ç®¡ç† | `config/config.go:1` |
| æŒ‡æ ‡å®šä¹‰ | `metrics/metrics.go:1` |
| æ•°æ®åº“ç›‘æ§ | `collectors/database.go:1` |
| K8s é›†ç¾¤ç›‘æ§ | `collectors/kubernetes.go:1` |
| é•œåƒä»“åº“ç›‘æ§ | `collectors/registry.go:1` |
| å¯¹è±¡å­˜å‚¨ç›‘æ§ | `collectors/storage.go:1` |
| ç£ç›˜ç©ºé—´ç›‘æ§ | `collectors/disk.go:1` |
| è®¡ç®—èµ„æºç›‘æ§ | `collectors/resources.go:1` |
| èŠ‚ç‚¹çŠ¶æ€ç›‘æ§ | `collectors/node.go:1` |
| K8s éƒ¨ç½²é…ç½® | `deploy/kubernetes/deploy.yaml:1` |
| Prometheus å‘Šè­¦è§„åˆ™ | `deploy/kubernetes/prometheus-rules.yaml:1` |

---

## é™„å½•

### A. ç¯å¢ƒå˜é‡å®Œæ•´åˆ—è¡¨

| ç¯å¢ƒå˜é‡ | ç±»å‹ | é»˜è®¤å€¼ | è¯´æ˜ |
|---------|------|-------|------|
| `METRICS_PORT` | int | 9090 | Metrics æœåŠ¡ç«¯å£ |
| `COLLECT_INTERVAL` | duration | 30s | é‡‡é›†é—´éš” |
| `IN_CLUSTER` | bool | true | æ˜¯å¦è¿è¡Œåœ¨ K8s é›†ç¾¤å†… |
| `GRDATA_PATH` | string | /grdata | /grdata ç›®å½•è·¯å¾„ |
| `NODE_CPU_THRESHOLD` | float64 | 80.0 | èŠ‚ç‚¹ CPU é«˜è´Ÿè½½é˜ˆå€¼ï¼ˆ%ï¼‰ |
| `NODE_MEMORY_THRESHOLD` | float64 | 80.0 | èŠ‚ç‚¹å†…å­˜é«˜è´Ÿè½½é˜ˆå€¼ï¼ˆ%ï¼‰ |
| `DB_{N}_NAME` | string | - | æ•°æ®åº“å®ä¾‹åç§° |
| `DB_{N}_HOST` | string | - | æ•°æ®åº“ä¸»æœºåœ°å€ |
| `DB_{N}_PORT` | int | 3306 | æ•°æ®åº“ç«¯å£ |
| `DB_{N}_USER` | string | root | æ•°æ®åº“ç”¨æˆ·å |
| `DB_{N}_PASSWORD` | string | - | æ•°æ®åº“å¯†ç  |
| `DB_{N}_DATABASE` | string | mysql | æ•°æ®åº“å |
| `REGISTRY_{N}_NAME` | string | - | é•œåƒä»“åº“å®ä¾‹åç§° |
| `REGISTRY_{N}_URL` | string | - | é•œåƒä»“åº“ URL |
| `REGISTRY_{N}_USER` | string | - | é•œåƒä»“åº“ç”¨æˆ·å |
| `REGISTRY_{N}_PASSWORD` | string | - | é•œåƒä»“åº“å¯†ç  |
| `REGISTRY_{N}_INSECURE` | bool | false | æ˜¯å¦å…è®¸ä¸å®‰å…¨è¿æ¥ |
| `MINIO_ENDPOINT` | string | - | MinIO/S3 ç«¯ç‚¹ |
| `MINIO_ACCESS_KEY` | string | - | MinIO è®¿é—®å¯†é’¥ |
| `MINIO_SECRET_KEY` | string | - | MinIO å¯†é’¥ |
| `MINIO_USE_SSL` | bool | false | æ˜¯å¦ä½¿ç”¨ SSL |

---

### B. ä¾èµ–åŒ…ç‰ˆæœ¬

```
go 1.21

require (
    github.com/go-sql-driver/mysql v1.9.3
    github.com/minio/minio-go/v7 v7.0.97
    github.com/prometheus/client_golang v1.23.2
    k8s.io/api v0.28.4
    k8s.io/apimachinery v0.28.4
    k8s.io/client-go v0.28.4
    k8s.io/metrics v0.28.4
)
```

---

**æ–‡æ¡£ç‰ˆæœ¬**: 1.0
**æœ€åæ›´æ–°**: 2025-12-19
**ç»´æŠ¤è€…**: Rainbond Platform Team
